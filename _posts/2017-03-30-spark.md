---
layout: post
title: Spark学习总结
---

***


[本人JVM讲解](https://nanhuirong.github.io/JVM)

### Hadoop与Spark的区别
1.二者均为大数据框架，但是Spark需要一个分布式第三方存储。
2.Spark的优势在于速度。
3.Spark在高级数据处理（实时流处理、机器学习）方面强于Hadoop（需要第三方的Apache Mahout）。


### Spark Job概念详解

![spark Yarn运行模式](https://raw.githubusercontent.com/nanhuirong/nanhuirong.github.io/master/_posts/spark/spark部署图.png)<br>

1.部署图解
+ 每个Worker上存在一个或者多个ExecutorBackend进程，每个进程包含一个Executor对象，该对象拥有一个线程池，每一个线程可以执行一个task
+ 在Standalone中，每一个ExecutorBackend被实例化成一个CoarseGrainedExecutorBackend进程

2.计算过程
+ 根据action将一个Application划分成多个作业（Job），一个job会被切分成若干Stage（根据shuffle依赖），一个Application会被切分成多个Job，一个Job会被切分成若干Stage，一个Stage会被切分成若干Task<br>
+ 一个作业经DAG调度和任务调度，被划分成一个个任务对应Task类<br>
+ 任务被分配到不同的核心区执行，Task.run<br>
+ Task.run调用阶段末RDD的iterator，获取该RDD某个分区的记录，iterator可能调用RDD.compute方法（负责父RDD与子RDD的计算逻辑）<br>

3.Stage与Task划分
+ 从后往前推，遇到ShuffleDependency就断开，遇到NarrowDependency就将其加入该Stage每一个stage中tassk数目由最后一个RDD的partitions决定<br>
+ Stage最后要产生result则task都是ResultTask，否则都是ShuffleMapTask<br>
+ 一个job会首先提交没有父stage的stage<br>

### Spark RDD源码拜读
1.RDD分区
>在RDD内部，使用分区来表示并行计算的一个单元，分区的个数决定了并行计算的力度。<br>

+ 分区实现<br>

```scala
//index：表示该分区在RDD内部的编号。通过RDD编号+分区编号可以唯一确定分区对应的块编号
trait Partition extends Serializable {
  /**
   * Get the partition's index within its parent RDD
   */
  def index: Int

  // A better default implementation of HashCode
  override def hashCode(): Int = index

  override def equals(other: Any): Boolean = super.equals(other)
}
```

```scala
//RDD抽象类中与分区相关的内容
@transient private var partitions_ : Array[Partition] = null
protected def getPartitions: Array[Partition]

  final def partitions: Array[Partition] = {
    checkpointRDD.map(_.partitions).getOrElse {
      if (partitions_ == null) {
        //getPartitions方法由子类实现
        partitions_ = getPartitions
        partitions_.zipWithIndex.foreach { case (partition, index) =>
          require(partition.index == index,
            s"partitions($index).partition == ${partition.index}, but it should equal $index")
        }
      }
      partitions_
    }
  }
```

+ 分区个数<br>
>分区的分配原则：尽量使得分区的个数==集群的核心数目<br>
>RDD可以被创建和转换得到，在转换操作中分区个数由转换操作对应的多个RDD之间的依赖关系确定<br>
>parallelize，在内部尝试将输入的数组做一次均匀分配<br>

```scala
/** Distribute a local Scala collection to form an RDD.
   *
   * @note Parallelize acts lazily. If `seq` is a mutable collection and is altered after the call
   * to parallelize and before the first action on the RDD, the resultant RDD will reflect the
   * modified collection. Pass a copy of the argument to avoid this.
   * @note avoid using `parallelize(Seq())` to create an empty `RDD`. Consider `emptyRDD` for an
   * RDD with no partitions, or `parallelize(Seq[T]())` for an RDD of `T` with empty partitions.
    * 默认情况下分区的个数会==spark.default.parallelism
    * （官方的解释：用于控制Shuffle过程中的默认使用的任务数）
    * 如果该值未设置则会根据不同集群的特征确定该值（本地模式为CPU核心数或者Local[N]）
   */
  def parallelize[T: ClassTag](
      seq: Seq[T],
      numSlices: Int = defaultParallelism): RDD[T] = withScope {
    assertNotStopped()
    new ParallelCollectionRDD[T](this, seq, numSlices, Map[Int, Seq[String]]())
  }

  def defaultParallelism: Int = {
    assertNotStopped()
    taskScheduler.defaultParallelism
  }

//LocalSchedulerBackend 本地模式
  override def defaultParallelism(): Int =
    scheduler.conf.getInt("spark.default.parallelism", totalCores)

//MesosFineGrainedSchedulerBackend Mesos模式
override def defaultParallelism(): Int =
    sc.conf.getInt("spark.default.parallelism", 8)

//CoarseGrainedSchedulerBackend Yarn或者standalone
  override def defaultParallelism(): Int = {
    conf.getInt("spark.default.parallelism", math.max(totalCoreCount.get(), 2))
  }

```

```scala
//textFile，默认是2
def defaultMinPartitions: Int = math.min(defaultParallelism, 2)

  def textFile(
      path: String,
      minPartitions: Int = defaultMinPartitions): RDD[String] = withScope {
    assertNotStopped()
    hadoopFile(path, classOf[TextInputFormat], classOf[LongWritable], classOf[Text],
      minPartitions).map(pair => pair._2.toString).setName(path)
  }
```

+ 分区内部记录数<br>
>分区原则：尽可能使同一 RDD 不同分区内的记录的数量一致<br>
>transform：窄依赖的分区记录数量依赖于父RDD中相同分区编号是如何进行数据分配;Shuffle依赖分区记录数依赖于选择的分区器，（哈希分区无法保证数据被平均分配到各个分区，范围分区可以保证数据分布均匀）<br>
>方法分区内数据的大小则是由 Hadoop API 接口 FileInputFormat.getSplits方法决定（见 HadoopRDD 类），得到的每一个分片即为 RDD 的一个分区，分片内数据的大小会受文件大小、文件是否可分割、HDFS 中块大小等因素的影响，但总体而言会是比较均衡的分配。<br>

+ 分区器<br>

```scala
//哈希分区,缺点：其散列到不同范围的概率会因数据而异
class HashPartitioner(partitions: Int) extends Partitioner {
  require(partitions >= 0, s"Number of partitions ($partitions) cannot be negative.")

  def numPartitions: Int = partitions

  def getPartition(key: Any): Int = key match {
    case null => 0
    case _ => Utils.nonNegativeMod(key.hashCode, numPartitions)
  }

  override def equals(other: Any): Boolean = other match {
    case h: HashPartitioner =>
      h.numPartitions == numPartitions
    case _ =>
      false
  }

  override def hashCode: Int = numPartitions
}
```


>2）范围分区<br>
>数据分布均衡，并且分区内数据的上界有序<br>

2.RDD依赖
>RDD的容错机制通过记录（记录的信息被称为血统lineage，在源码中表现为依赖关系）更新来实现，且记录是粗粒度的转换操作<br>
>依赖只保存父RDD信息，对应Dependency抽象类，分为窄依赖和宽依赖（又称Shuffle依赖）<br>

```scala
abstract class Dependency[T] extends Serializable {
  // 对应父RDD，一次转换涉及多个父RDD，就会产生多个Dependency对象
  // 所有的Dependency对象存储在子RDD内部
  def rdd: RDD[T]
}
```

+ 窄依赖

```scala
//一个父RDD的分区至多被一个子RDD的分区使用，意味着父RDD分区内的数据不能被分割
abstract class NarrowDependency[T](_rdd: RDD[T]) extends Dependency[T] {
  /**
   * Get the parent partitions for a child partition.
   * @param partitionId a partition of the child RDD
   */
  // 用于获取一个分区数据来源于父RDD的那些分区
  // 进一步分成OneToOneDependency（一对一依赖） RangeDependency（范围依赖）
  def getParents(partitionId: Int): Seq[Int]

  override def rdd: RDD[T] = _rdd
}

class OneToOneDependency[T](rdd: RDD[T]) extends NarrowDependency[T](rdd) {
  // 一对一依赖表示子RDD分区编号与父RDD分区编号完全一致
  override def getParents(partitionId: Int): List[Int] = List(partitionId)
}

/**
 * :: DeveloperApi ::
 * Represents a one-to-one dependency between ranges of partitions in the parent and child RDDs.
 * @param rdd the parent RDD
 * @param inStart the start of the range in the parent RDD
 * @param outStart the start of the range in the child RDD
 * @param length the length of the range
 */
@DeveloperApi
class RangeDependency[T](rdd: RDD[T], inStart: Int, outStart: Int, length: Int)
  extends NarrowDependency[T](rdd) {
  // 只能被用于表示UnionRDD与父RDD的依赖关系，除了第一个父RDD其他父RDD与子RDD的分区编号不再一致
  override def getParents(partitionId: Int): List[Int] = {
    if (partitionId >= outStart && partitionId < outStart + length) {
      List(partitionId - outStart + inStart)
    } else {
      Nil
    }
  }
}

```

+ Shuffle依赖
>一个父RDD的分区可能被多个子RDD分区使用，意味着父RDD与子RDD之间存在Shuffle关系<br>

```scala
class ShuffleDependency[K: ClassTag, V: ClassTag, C: ClassTag](
    // 子RDD依赖的父RDD
    @transient private val _rdd: RDD[_ <: Product2[K, V]],
    // 分区器，决定shuffle过程中reducer的个数
    // （子RDD的分区数以及map端的一条记录如何分配给一个reducer）
    //  CoGroupedRDD 中，确定父 RDD 与子 RDD 之间的依赖关系类型。
    val partitioner: Partitioner,
    // 序列化器，Mapper端的序列化和Reducer端的反序列化
    val serializer: Serializer = SparkEnv.get.serializer,
    // 键值排序策略，决定子RDD一个分区内部键值的排序
    val keyOrdering: Option[Ordering[K]] = None,
    // 聚合器，内部包含多个聚合函数
    val aggregator: Option[Aggregator[K, V, C]] = None,
    // 指定shuffle过程中是否需要在map端进行combine，在groubByKey中为false，reduceByKey中为true
    val mapSideCombine: Boolean = false)
  extends Dependency[Product2[K, V]] {

  override def rdd: RDD[Product2[K, V]] = _rdd.asInstanceOf[RDD[Product2[K, V]]]

  private[spark] val keyClassName: String = reflect.classTag[K].runtimeClass.getName
  private[spark] val valueClassName: String = reflect.classTag[V].runtimeClass.getName
  // Note: It's possible that the combiner class tag is null, if the combineByKey
  // methods in PairRDDFunctions are used instead of combineByKeyWithClassTag.
  private[spark] val combinerClassName: Option[String] =
    Option(reflect.classTag[C]).map(_.runtimeClass.getName)
  // shuffle编号，在一个Application中shuffle编号是唯一的
  val shuffleId: Int = _rdd.context.newShuffleId()
  // shuffle句柄，（shuffleID，Mapper的个数，对应的Shuffle依赖）
  val shuffleHandle: ShuffleHandle = _rdd.context.env.shuffleManager.registerShuffle(
    shuffleId, _rdd.partitions.length, this)

  _rdd.sparkContext.cleaner.foreach(_.registerShuffleForCleanup(this))
}
```

>Spark 将计算链从Shuffle依赖处断开，划分成不同的Stage（意味着不同的stage之间是shuffle依赖）
。从而构建一张不同Stage之间的DAG（有向无环图）<br>

3.计算函数
>计算过程：<br>
> 1）根据action将一个Application划分成多个作业<br>
> 2）一个作业经DAG调度和任务调度，被划分成一个个任务对应Task类<br>
> 3）任务被分配到不同的核心区执行，Task.run<br>
> 4）Task.run调用阶段末RDD的iterator，获取该RDD某个分区的记录，iterator可能
调用RDD.compute方法（负责父RDD与子RDD的计算逻辑）<br>

+ compute方法：负责父RDD到子RDD分区数据的变换逻辑

```scala
private[spark] class MapPartitionsRDD[U: ClassTag, T: ClassTag](
    var prev: RDD[T],
    f: (TaskContext, Int, Iterator[T]) => Iterator[U],  // (TaskContext, partition index, iterator)
    preservesPartitioning: Boolean = false)
  extends RDD[U](prev) {

  override val partitioner = if (preservesPartitioning) firstParent[T].partitioner else None

  override def getPartitions: Array[Partition] = firstParent[T].partitions

  override def compute(split: Partition, context: TaskContext): Iterator[U] =
    // firstParent在抽象类RDD中定义, iterator方法返回一个iterator对象
    // 内部存储的数据是父RDD计算完的数据
    f(context, split.index, firstParent[T].iterator(split, context))

  override def clearDependencies() {
    super.clearDependencies()
    prev = null
  }
}



//RDD
  /** Returns the first parent RDD */
  protected[spark] def firstParent[U: ClassTag]: RDD[U] = {
    dependencies.head.rdd.asInstanceOf[RDD[U]]
  }

  //f 在RDD的map方法中指定
  /**
   * Return a new RDD by applying a function to all elements of this RDD.
   */
  def map[U: ClassTag](f: T => U): RDD[U] = withScope {
    val cleanF = sc.clean(f)
    new MapPartitionsRDD[U, T](this, (context, pid, iter) => iter.map(cleanF))
  }

```

+ iterator方法

```scala
//RDD
  final def iterator(split: Partition, context: TaskContext): Iterator[T] = {
    // 首先检查当前的存储级别，如果补位None说明分区数据要么已经存储在文件系统
    // ，要么已经执行过持久化操作
    if (storageLevel != StorageLevel.NONE) {
      getOrCompute(split, context)
    } else {
      computeOrReadCheckpoint(split, context)
    }
  }

  private[spark] def computeOrReadCheckpoint(split: Partition, context: TaskContext): Iterator[T] =
  {
    // 检查当前RDD是否被设置成检查点对于标记成检查点的情况，当前 RDD 的父 RDD 不再是原先转换操作中提供数据的父 RDD，
    // 而是被 Apache Spark 替换成一个 CheckpointRDD 对象，该对象中的数据存放在文件系统中，
    // 因此最终该对象会从文件系统中读取数据并返回给 computeOrReadCheckpoint 方法
    if (isCheckpointedAndMaterialized) {
      firstParent[T].iterator(split, context)
    } else {
      compute(split, context)
    }
  }

  // 根据RDD编号和分区编号计算当前分区在存储层对用的blockID，通过存储层提供的读取接口读取块数据
  // 1）数据之前已经在存储介质，成功提取并返回
  // 2）数据不在存储介质，可能数据已经丢失，或者RDD经过持久化，当前分区是第一次被计算，
  // 计算分区数据并且持久化
  private[spark] def getOrCompute(partition: Partition, context: TaskContext): Iterator[T] = {
    val blockId = RDDBlockId(id, partition.index)
    var readCachedBlock = true
    // This method is called on executors, so we need call SparkEnv.get instead of sc.env.
    SparkEnv.get.blockManager.getOrElseUpdate(blockId, storageLevel, elementClassTag, () => {
      readCachedBlock = false
      computeOrReadCheckpoint(partition, context)
    }) match {
      case Left(blockResult) =>
        if (readCachedBlock) {
          val existingMetrics = context.taskMetrics().inputMetrics
          existingMetrics.incBytesRead(blockResult.bytes)
          new InterruptibleIterator[T](context, blockResult.data.asInstanceOf[Iterator[T]]) {
            override def next(): T = {
              existingMetrics.incRecordsRead(1)
              delegate.next()
            }
          }
        } else {
          new InterruptibleIterator(context, blockResult.data.asInstanceOf[Iterator[T]])
        }
      case Right(iter) =>
        new InterruptibleIterator(context, iter.asInstanceOf[Iterator[T]])
    }
  }
```

### SparkContext源码阅读

![](https://raw.githubusercontent.com/nanhuirong/nanhuirong.github.io/master/_posts/spark/Scheduler.png)

1.TaskScheduler（与SchedulerBackend成对出现）
+ 本地模式<br>
>TaskScheduler对应为TaskSchedulerImpl，SchedulerBackend（负责与底层资源调度器进行交互）对应为LocalSchedulerBackend<br>
+ Standalone模式<br>
>TaskScheduler对应为TaskSchedulerImpl，SchedulerBackend对应为StandaloneSchedulerBackend（继承自CoarseGrainedSchedulerBackend（粗粒度的资源调度类））<br>
+ Yarn-cluster<br>
>YarnClientScheduler（TaskSchedulerImpl的子类），YarnClusterSchedulerBackend（继承自继承自CoarseGrainedSchedulerBackend）。<br>
+ Yarn-client<br>
>会在集群外部启动一个ExecutorLauncher作为Driver<br>
+ Mesos<br>
>分为粗粒度和细粒度。<br>

### Spark性能调优及数据倾斜
>Spark性能调优包含：开发调优、资源调优、数据倾斜调优、shuffle调优。<br>

1.开发调优
+ 避免重复创建RDD

```scala
val data = sc.textFile(PATH)
data.map()
val data1 = sc.textFile(PATH)
data1.filter()
//正确的写法
val data = sc.textFile(PATH)
data.map()
data.filter()
```

+ 尽可能复用同一个RDD
+ 对多次使用的RDD进行持久化
>MEMORY_ONLY：使用未序列化的Java对象格式，将数据保存在内存中。如果内存不够存放所有的数据，则数据可能就不会进行持久化。那么下次对这个RDD执行算子操作时，那些没有被持久化的数据，需要从源头处重新计算一遍。这是默认的持久化策略，使用cache()方法时，实际就是使用的这种持久化策略。<br>
>MEMORY_AND_DISK：使用未序列化的Java对象格式，优先尝试将数据保存在内存中。如果内存不够存放所有的数据，会将数据写入磁盘文件中，下次对这个RDD执行算子时，持久化在磁盘文件中的数据会被读取出来使用。<br>
>MEMORY_ONLY_SER：基本含义同MEMORY_ONLY。唯一的区别是，会将RDD中的数据进行序列化，RDD的每个partition会被序列化成一个字节数组。这种方式更加节省内存，从而可以避免持久化的数据占用过多内存导致频繁GC。<br>
>MEMORY_AND_DISK_SER：基本含义同MEMORY_AND_DISK。唯一的区别是，会将RDD中的数据进行序列化，RDD的每个partition会被序列化成一个字节数组。这种方式更加节省内存，从而可以避免持久化的数据占用过多内存导致频繁GC。<br>
>DISK_ONLY：使用未序列化的Java对象格式，将数据全部写入磁盘文件中。<br>
>MEMORY_ONLY_2, MEMORY_AND_DISK_2：对于上述任意一种持久化策略，如果加上后缀_2，代表的是将每个持久化的数据，都复制一份副本，并将副本保存到其他节点上。这种基于副本的持久化机制主要用于进行容错。假如某个节点挂掉，节点的内存或磁盘中的持久化数据丢失了，那么后续对RDD计算时还可以使用该数据在其他节点上的副本。如果没有副本的话，就只能将这些数据从源头处重新计算一遍了。<br>

+ 避免使用Shuffle类算子<br>
>能避免则尽可能避免使用reduceByKey、join、distinct、repartition等会进行shuffle的算子，尽量使用map类的非shuffle算子。<br>
>利用Broadcast+map替代join操作<br>

```scala
//传统写法
val rdd3 = rdd2.join(rdd1)
//改进，利用Broadcast+map，不会导致Shuffle
val rdd3 = rdd2.collect()
val broadcast = sc.broadcast(rdd2)
val rdd4 = rdd1.map(broadcast)
//仅仅建议在rdd2比较小的时候使用（百兆、1G-2G），适当的Driver的内存应该大一些。
//而且每个Executor会保留一份rdd2的数据

```

+ 使用map-side预聚合的shuffle操作（相当于Hadoop的Combiner，又称本地reduce）<br>
>建议使用reduceByKey、aggregateByKey替代groupByKey。因为groupByKey不会进行本地reduce<br>

+ 使用高性能的算子
>reduceByKey、aggregateByKey替代groupBykey<br>
>使用mapPartition替代map：mapPartition一个函数会处理一个partition的所有数据（比如处理数据库链接操作），同时可能出现OOM，需要谨慎使用。<br>
>使用foreachPartition替代foreach：与上面类似。<br>
>filter之后进行coalesce操作：其实就是手动触发一次数据分区操作，减少partition数目。<br>
>使用repartitionAndSortWithinPartitions替代repartition+sort<br>
>使用广播变量广播大变量<br>
>使用Kryo优化序列化性能<br>
> 在算子函数中使用到外部变量时，该变量会被序列化后进行网络传输<br>
> 将自定义的类型作为RDD的泛型类型时（比如JavaRDD，Student是自定义类型），所有自定义类型对象，都会进行序列化。因此这种情况下，也要求自定义的类必须实现Serializable接口<br>
> 使用可序列化的持久化策略时（比如MEMORY_ONLY_SER），Spark会将RDD中的每个partition都序列化成一个大的字节数组。<br>

+ 优化数据结构
>java中三种耗费内存类型：<br>
> 对象，每个Java对象都有对象头、引用等额外的信息，因此比较占用内存空间。<br>
> 字符串，每个字符串内部都有一个字符数组以及长度等额外信息。<br>
> 集合类型，比如HashMap、LinkedList等，因为集合类型内部通常会使用一些内部类来封装集合元素，比如Map.Entry<br>
建议使用字符串代替对象，使用原始数据类型替代字符串，使用数组替代集合类型，尽力而为，不然没法写代码。<br>

2.资源调优<br>
包含三种模式：standalone、yarn、mesos<br>

![spark Yarn运行模式](https://raw.githubusercontent.com/nanhuirong/nanhuirong.github.io/master/_posts/spark/spark运行模式.png)

+ Spark yarn模式运行原理
>我们使用spark-submit提交一个Spark作业之后，这个作业就会启动一个对应的Driver进程。根据你使用的部署模式（deploy-mode）不同，Driver进程可能在本地启动，也可能在集群中某个工作节点上启动。Driver进程本身会根据我们设置的参数，占有一定数量的内存和CPU core。而Driver进程要做的第一件事情，就是向集群管理器（可以是Spark Standalone集群，也可以是其他的资源管理集群，美团•大众点评使用的是YARN作为资源管理集群）申请运行Spark作业需要使用的资源，这里的资源指的就是Executor进程。YARN集群管理器会根据我们为Spark作业设置的资源参数，在各个工作节点上，启动一定数量的Executor进程，每个Executor进程都占有一定数量的内存和CPU core。<br>
>在申请到了作业执行所需的资源之后，Driver进程就会开始调度和执行我们编写的作业代码了。Driver进程会将我们编写的Spark作业代码分拆为多个stage，每个stage执行一部分代码片段，并为每个stage创建一批task，然后将这些task分配到各个Executor进程中执行。task是最小的计算单元，负责执行一模一样的计算逻辑（也就是我们自己编写的某个代码片段），只是每个task处理的数据不同而已。一个stage的所有task都执行完毕之后，会在各个节点本地的磁盘文件中写入计算中间结果，然后Driver就会调度运行下一个stage。下一个stage的task的输入数据就是上一个stage输出的中间结果。如此循环往复，直到将我们自己编写的代码逻辑全部执行完，并且计算完所有的数据，得到我们想要的结果为止。
>Spark是根据shuffle类算子来进行stage的划分。如果我们的代码中执行了某个shuffle类算子（比如reduceByKey、join等），那么就会在该算子处，划分出一个stage界限来。可以大致理解为，shuffle算子执行之前的代码会被划分为一个stage，shuffle算子执行以及之后的代码会被划分为下一个stage。因此一个stage刚开始执行的时候，它的每个task可能都会从上一个stage的task所在的节点，去通过网络传输拉取需要自己处理的所有key，然后对拉取到的所有相同的key使用我们自己编写的算子函数执行聚合操作（比如reduceByKey()算子接收的函数）。这个过程就是shuffle。<br>
>当我们在代码中执行了cache/persist等持久化操作时，根据我们选择的持久化级别的不同，每个task计算出来的数据也会保存到Executor进程的内存或者所在节点的磁盘文件中。<br>
>因此Executor的内存主要分为三块：第一块是让task执行我们自己编写的代码时使用，默认是占Executor总内存的20%；第二块是让task通过shuffle过程拉取了上一个stage的task的输出后，进行聚合等操作时使用，默认也是占Executor总内存的20%；第三块是让RDD持久化时使用，默认占Executor总内存的60%。<br>
>task的执行速度是跟每个Executor进程的CPU core数量有直接关系的。一个CPU core同一时间只能执行一个线程。而每个Executor进程上分配到的多个task，都是以每个task一条线程的方式，多线程并发运行的。如果CPU core数量比较充足，而且分配到的task数量比较合理，那么通常来说，可以比较快速和高效地执行完这些task线程。<br>

+ 调优参数
>num-executors:<br>
> 参数说明：该参数用于设置Spark作业总共要用多少个Executor进程来执行。Driver在向YARN集群管理器申请资源时，YARN集群管理器会尽可能按照你的设置来在集群的各个工作节点上，启动相应数量的Executor进程。这个参数非常之重要，如果不设置的话，默认只会给你启动少量的Executor进程，此时你的Spark作业的运行速度是非常慢的。<br>
> 参数调优建议：每个Spark作业的运行一般设置50~100个左右的Executor进程比较合适，设置太少或太多的Executor进程都不好。设置的太少，无法充分利用集群资源；设置的太多的话，大部分队列可能无法给予充分的资源。<br>
>executor-memory<br>
> 参数说明：该参数用于设置每个Executor进程的内存。Executor内存的大小，很多时候直接决定了Spark作业的性能，而且跟常见的JVM OOM异常，也有直接的关联。<br>
> 参数调优建议：每个Executor进程的内存设置4G~8G较为合适。但是这只是一个参考值，具体的设置还是得根据不同部门的资源队列来定。可以看看自己团队的资源队列的最大内存限制是多少，num-executors乘以executor-memory，就代表了你的Spark作业申请到的总内存量（也就是所有Executor进程的内存总和），这个量是不能超过队列的最大内存量的。此外，如果你是跟团队里其他人共享这个资源队列，那么申请的总内存量最好不要超过资源队列最大总内存的1/3~1/2，避免你自己的Spark作业占用了队列所有的资源，导致别的同学的作业无法运行。<br>
>executor-cores<br>
> 参数说明：该参数用于设置每个Executor进程的CPU core数量。这个参数决定了每个Executor进程并行执行task线程的能力。因为每个CPU core同一时间只能执行一个task线程，因此每个Executor进程的CPU core数量越多，越能够快速地执行完分配给自己的所有task线程。<br>
> 参数调优建议：Executor的CPU core数量设置为2~4个较为合适。同样得根据不同部门的资源队列来定，可以看看自己的资源队列的最大CPU core限制是多少，再依据设置的Executor数量，来决定每个Executor进程可以分配到几个CPU core。同样建议，如果是跟他人共享这个队列，那么num-executors * executor-cores不要超过队列总CPU core的1/3~1/2左右比较合适，也是避免影响其他同学的作业运行。<br>
>driver-memory<br>
> 参数说明：该参数用于设置Driver进程的内存。<br>
> 参数调优建议：Driver的内存通常来说不设置，或者设置1G左右应该就够了。唯一需要注意的一点是，如果需要使用collect算子将RDD的数据全部拉取到Driver上进行处理，那么必须确保Driver的内存足够大，否则会出现OOM内存溢出的问题。<br>
>spark.default.parallelism<br>
> 参数说明：该参数用于设置每个stage的默认task数量。这个参数极为重要，如果不设置可能会直接影响你的Spark作业性能。<br>
> 参数调优建议：Spark作业的默认task数量为500~1000个较为合适。很多同学常犯的一个错误就是不去设置这个参数，那么此时就会导致Spark自己根据底层HDFS的block数量来设置task的数量，默认是一个HDFS block对应一个task。通常来说，Spark默认设置的数量是偏少的（比如就几十个task），如果task数量偏少的话，就会导致你前面设置好的Executor的参数都前功尽弃。试想一下，无论你的Executor进程有多少个，内存和CPU有多大，但是task只有1个或者10个，那么90%的Executor进程可能根本就没有task执行，也就是白白浪费了资源！因此Spark官网建议的设置原则是，设置该参数为num-executors * executor-cores的2~3倍较为合适，比如Executor的总CPU core数量为300个，那么设置1000个task是可以的，此时可以充分地利用Spark集群的资源。<br>
>spark.storage.memoryFraction<br>
> 参数说明：该参数用于设置RDD持久化数据在Executor内存中能占的比例，默认是0.6。也就是说，默认Executor 60%的内存，可以用来保存持久化的RDD数据。根据你选择的不同的持久化策略，如果内存不够时，可能数据就不会持久化，或者数据会写入磁盘。<br>
> 参数调优建议：如果Spark作业中，有较多的RDD持久化操作，该参数的值可以适当提高一些，保证持久化的数据能够容纳在内存中。避免内存不够缓存所有的数据，导致数据只能写入磁盘中，降低了性能。但是如果Spark作业中的shuffle类操作比较多，而持久化操作比较少，那么这个参数的值适当降低一些比较合适。此外，如果发现作业由于频繁的gc导致运行缓慢（通过spark web ui可以观察到作业的gc耗时），意味着task执行用户代码的内存不够用，那么同样建议调低这个参数的值。<br>
>spark.shuffle.memoryFraction<br>
> 参数说明：该参数用于设置shuffle过程中一个task拉取到上个stage的task的输出后，进行聚合操作时能够使用的Executor内存的比例，默认是0.2。也就是说，Executor默认只有20%的内存用来进行该操作。shuffle操作在进行聚合时，如果发现使用的内存超出了这个20%的限制，那么多余的数据就会溢写到磁盘文件中去，此时就会极大地降低性能。<br>
> 参数调优建议：如果Spark作业中的RDD持久化操作较少，shuffle操作较多时，建议降低持久化操作的内存占比，提高shuffle操作的内存占比比例，避免shuffle过程中数据过多时内存不够用，必须溢写到磁盘上，降低了性能。此外，如果发现作业由于频繁的gc导致运行缓慢，意味着task执行用户代码的内存不够用，那么同样建议调低这个参数的值。<br>

```shell
./bin/spark-submit \
  --master yarn-cluster \
  --num-executors 100 \
  --executor-memory 6G \
  --executor-cores 4 \
  --driver-memory 1G \
  --conf spark.default.parallelism=1000 \
  --conf spark.storage.memoryFraction=0.5 \
  --conf spark.shuffle.memoryFraction=0.3 \
```

### 数据倾斜

```scala
//查看Key分布源代码
val sample = rdd.sample(false, 0.1)
val count = sample.countByKey()
counts.foreach(println)
```

1.现象及原因
+ 现象<br>
>并行处理的数据集，某一部分（一个或者多个partition）的数据明显多于其他部分，
从而使该部分的处理速度成为瓶颈<br>
+ 对应的算子<br>
>distinct、reduceByKey、aggregateByKey、join、cogroup、repartition。<br>

2.解决方案
+ 调整并行度（并不一定是增加）分散同一个Task的不同key<br>
>Spark在Shuffle过程中，默认使用HashPartitioner进行数据分区。<br>
>该方案只能缓解数据倾斜造成的瓶颈问题。<br>
>设置spark.default.parallelism或者直接在shuffle算子上直接设置并行度。<br>
>劣势：只能将分配到同一Task的不同Key散开，对于同一Key倾斜险种的情况无法解决。<br>
+ 自定义Partitioner（默认为HashPartitioner）<br>
>劣势：与调整并行度相同。<br>
+ 将Reduce side join转化为Map side Join<br>
>使用广播变量将小表广播到每一个executor。<br>
>适用于小表数据量足够小的情况，并且为Join场景。<br>
+ 为倾斜的key增加随机前缀或者随机后缀<br>
>原理：为倾斜key增加随机数，使其分散到不同的Task，在Join侧对倾斜key做笛卡尔乘积保证与其正常进行join<br>
>实现：将leftRDD的倾斜Key加上随机数[0, 100]形成leftSkewRDD，
将rightRDD中对应的倾斜Key每一条都转换成[0, 100]共100条形成rightSknewRDD，
将leftSkewRDD与rightSkewRDD Join形成SkewJoinRDD，
将leftRDD中不包含倾斜key的数据与rightRDD做join，
将二者进行union。<br>
>适用场景：两张表都比较大，其中一个RDD的少数Key数据倾斜，另外一个RDD数据分布均匀。<br>
>如果倾斜RDD的倾斜Key非常多，另一个RDD膨胀非常大。<br>
+ 大表增加N种随机前缀，小表扩大N倍<br>
>原理：对倾斜RDD每一个key增加随机前缀，将另外一个RDD扩大N倍。<br>
>适用场景：一个数据集倾斜Key太多，另外一个分布均匀。<br>

### shuffle调优
>大多数Spark作业的性能主要就是消耗在了shuffle环节，因为该环节包含了大量的磁盘IO、序列化、网络数据传输等操作。因此，如果要让作业的性能更上一层楼，就有必要对shuffle过程进行调优。但是也必须提醒大家的是，影响一个Spark作业性能的因素，主要还是代码开发、资源参数以及数据倾斜，shuffle调优只能在整个Spark的性能调优中占到一小部分而已。因此大家务必把握住调优的基本原则，千万不要舍本逐末。下面我们就给大家详细讲解shuffle的原理，以及相关参数的说明，同时给出各个参数的调优建议。<br>

1.shuffle调优参数<br>
+ spark.shuffle.file.buffer<br>
>默认值：32k<br>
>参数说明：该参数用于设置shuffle write task的BufferedOutputStream的buffer缓冲大小。
将数据写到磁盘文件之前，会先写入buffer缓冲中，待缓冲写满之后，才会溢写到磁盘。<br>
>调优建议：如果作业可用的内存资源较为充足的话，可以适当增加这个参数的大小（比如64k），
从而减少shuffle write过程中溢写磁盘文件的次数，也就可以减少磁盘IO次数，进而提升性能。
在实践中发现，合理调节该参数，性能会有1%~5%的提升。<br>
+ spark.reducer.maxSizeInFlight<br>
>默认值：48m<br>
>参数说明：该参数用于设置shuffle read task的buffer缓冲大小，
而这个buffer缓冲决定了每次能够拉取多少数据。<br>
>调优建议：如果作业可用的内存资源较为充足的话，
可以适当增加这个参数的大小（比如96m），从而减少拉取数据的次数，也就可以减少网络传输的次数，进而提升性能。
在实践中发现，合理调节该参数，性能会有1%~5%的提升。<br>
+ spark.shuffle.io.maxRetries<br>
>默认值：3<br>
>参数说明：shuffle read task从shuffle write task所在节点拉取属于自己的数据时，
如果因为网络异常导致拉取失败，是会自动进行重试的。该参数就代表了可以重试的最大次数。
如果在指定次数之内拉取还是没有成功，就可能会导致作业执行失败。<br>
>调优建议：对于那些包含了特别耗时的shuffle操作的作业，建议增加重试最大次数（比如60次），
以避免由于JVM的full gc或者网络不稳定等因素导致的数据拉取失败。在实践中发现，
对于针对超大数据量（数十亿~上百亿）的shuffle过程，调节该参数可以大幅度提升稳定性。<br>
+ spark.shuffle.io.retryWait<br>
>默认值：5s<br>
>参数说明：具体解释同上，该参数代表了每次重试拉取数据的等待间隔，默认是5s。<br>
>调优建议：建议加大间隔时长（比如60s），以增加shuffle操作的稳定性。<br>
+ spark.shuffle.memoryFraction<br>
>默认值：0.2<br>
>参数说明：该参数代表了Executor内存中，分配给shuffle read task进行聚合操作的内存比例，默认是20%。<br>
>调优建议：在资源参数调优中讲解过这个参数。如果内存充足，而且很少使用持久化操作，建议调高这个比例，
给shuffle read的聚合操作更多内存，以避免由于内存不足导致聚合过程中频繁读写磁盘。
在实践中发现，合理调节该参数可以将性能提升10%左右。<br>
+ spark.shuffle.manager<br>
>默认值：sort<br>
>参数说明：该参数用于设置ShuffleManager的类型。
Spark 1.5以后，有三个可选项：hash、sort和tungsten-sort。
HashShuffleManager是Spark 1.2以前的默认选项，
但是Spark 1.2以及之后的版本默认都是SortShuffleManager了。
tungsten-sort与sort类似，但是使用了tungsten计划中的堆外内存管理机制，内存使用效率更高。<br>
>调优建议：由于SortShuffleManager默认会对数据进行排序，因此如果你的业务逻辑中需要该排序机制的话，则使用默认的SortShuffleManager就可以；而如果你的业务逻辑不需要对数据进行排序，那么建议参考后面的几个参数调优，通过bypass机制或优化的HashShuffleManager来避免排序操作，同时提供较好的磁盘读写性能。这里要注意的是，tungsten-sort要慎用，因为之前发现了一些相应的bug。<br>
+ spark.shuffle.sort.bypassMergeThreshold<br>
>默认值：200<br>
>参数说明：当ShuffleManager为SortShuffleManager时，如果shuffle read task的数量小于这个阈值（默认是200），则shuffle write过程中不会进行排序操作，而是直接按照未经优化的HashShuffleManager的方式去写数据，但是最后会将每个task产生的所有临时磁盘文件都合并成一个文件，并会创建单独的索引文件。<br>
>调优建议：当你使用SortShuffleManager时，如果的确不需要排序操作，那么建议将这个参数调大一些，大于shuffle read task的数量。那么此时就会自动启用bypass机制，map-side就不会进行排序了，减少了排序的性能开销。但是这种方式下，依然会产生大量的磁盘文件，因此shuffle write性能有待提高。<br>
+ spark.shuffle.consolidateFiles<br>
>默认值：false<br>
>参数说明：如果使用HashShuffleManager，该参数有效。如果设置为true，那么就会开启consolidate机制，会大幅度合并shuffle write的输出文件，对于shuffle read task数量特别多的情况下，这种方法可以极大地减少磁盘IO开销，提升性能。<br>
>调优建议：如果的确不需要SortShuffleManager的排序机制，那么除了使用bypass机制，还可以尝试将spark.shffle.manager参数手动指定为hash，使用HashShuffleManager，同时开启consolidate机制。在实践中尝试过，发现其性能比开启了bypass机制的SortShuffleManager要高出10%~30%。<br>

2.ShuffleManager
>ShuffleManager 为shuffle管理器，负责shuffle过程的执行，计算和处理。在spark1.2之前默认的时HashShuffleManager，
会产生大量的中间磁盘文件，进而由大量磁盘IO影响性能。Spark1.2之后，默认的ShuffleManager改为SortShuffleManager，
改进：每个Task在进行Shuffle时，虽然会产生较多的临时磁盘文件，但是最终会合并成一个磁盘文件，因此一个Task只有一个磁盘文件，
在下一个Stage的Shuffle阶段拉取数据时根据索引读取磁盘文件中的部分数据。<br>
+ HashShuffleManager<br>
>先假设一个executor只有一个cpu core（意味着这个executor无论有多少个Task线程，同一时间只能执行一个）<br>
>原始HashShuffleManager：<br>
> shuffle write：一个Task会创建num个文件（num为后一个Stage的task数目），
比如下一个Stage有100个task，本stage有50个task，10个executor,
意味着一个executor会创建500个磁盘文件，总共创建5000个磁盘文件<br>
> shuffle read（边拉取边聚合，一个task一个buffer，一次只能拉取buffer大小的数据）：
每一个task拉取上游stage中所有task内部属于自己的磁盘文件，<br>
>优化后的HashShuffleManager（spark.shuffle.consolidateFiles设置为true）：<br>
> shuffle write：一个cpu core创建num个磁盘文件（下游task的数量），CPU core × num个磁盘文件<br>
+ SortShuffleManager<br>
>包含两种运行机制（普通和bypass运行机制，主要是是否进行排序），当shuffle read task的数量 <=
spark.shuffle.sort.bypassMergeThreshold（默认200）时启用bypass机制<br>
>普通机制：<br>
> shuffle write：数据写入一个内存数据结构（不同shuffle算子对应数据结构不同）
溢写磁盘时根据key对数据结构排序，默认的batch（10000）分批写入磁盘文件，
并将一个task的所有磁盘文件合并，并单独包含一个索引文件（包含下游task对应的start offset与end offset），<br>
>bypass机制（触发条件，超过阈值或者不是聚合类的算子比如reduceByKey）：<br>
> shuffle write：与未经优化的HashShuffleManager一样，只是最后加入一个merge的功能<br>
>二者的区别：bypass磁盘写机制不同，并且不会进行排序<br>

